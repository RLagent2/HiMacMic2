name: "SMAC-HiMacMic"

n_allgoals: 49
n_subgoals: 49
goal_dim: 49
x_range: 7.0
y_range: 7.0

agent_mac: "agent_mac"
agent: "hpn_rnn"
hpn_hyper_dim: 64
hpn_hyper_activation: 'relu'
hpn_head_num: 1 # 2 for 6h_vs_8z

epsilon_anneal_time: 100000 # 500000 for 6h_vs_8z
skip_epsilon_anneal_time: 100000 # 500000 for 6h_vs_8z
skip_dim: 4    #d
td_lambda: 0.6 #0.3 for 6h_vs_8z
beta: 0.01
count_lmd: 0.1
runner: "parallel"
batch_size_run: 8
r_baseline: 0.2
t_max: 8050000

size_run: 1
x_start: 5
y_start: 5
x_length: 15.0
y_length: 12.0
lr: 0.001 # Learning rate for agents
success_sig: 2
td_lambda2: 0.6

hindsight: False
success_hindsight: True
count_lmd_e: 0.0
counter_log_interval: 100000
tsg_sample_size: 20
buffer_size: 5000
tsg_buffer_size: 100
batch_size: 128 #can not train more than one batch
tsg_batch_size: 1 #add agent batch , add tsg train times
mini_batch: 4
# update the target network every {} episodes
target_update_interval: 200

# use epsilon greedy action selector
action_selector: "epsilon_greedy"
epsilon_start: 1.0
skip_epsilon_start: 1.0
epsilon_finish: 0.05
skip_epsilon_finish: 0.05

add_goal: True  #not use
reward_int: True
reward_positive: False #not use
add_goal_num: 0 #not use

obs_agent_id: True
obs_last_action: False
obs_individual_obs: False

# use the Q_Learner to train
tsg_mac: "ns_tsg_mac"
tsg_agent: "tsg_ns_rnn"
agent_output_type: q
learner: "tsg_learner"
mixer: "tsg_qmix"
agent_mixer: "qmix"
mixing_embed_dim: 32
hypernet_embed: 64
optimizer: 'adam'
q_lambda: False
skip_hidden_dim: 30 #not use
use_cuda: True
double_q: False

